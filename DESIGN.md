# Design Decisions

## Выбор языка и подхода

Я выбрал Python, поскольку знаком с ним на базовом уровне и имею опыт работы на нем.

## Как работает программа

1. **Чтение файла:**  
   Программа открывает входной файл, где каждое слово записано в отдельной строке.

2. **Подсчёт букв:**  
   С помощью `Counter` подсчитываем, сколько раз встречается каждая буква в слове. Т.е. для слова "cat" получается словарь `{'c': 1, 'a': 1, 't': 1}`.

3. **Группировка анаграмм:**  
   Для группировки анаграмм переводим результат `Counter` в `frozenset` – неизменяемый тип, который можно использовать как ключ в словаре. Если два слова дают одинаковый `frozenset`, значит, они содержат одни и те же буквы и принадлежат одной группе.

4. **Вывод результатов:**  
   Программа выводит каждую группу анаграмм в отдельной строке, где слова разделены пробелами.

## Замечания по масштабируемости

Согласно изученной мной информации, я могу сказать, что:
- **Для 10 миллионов слов:**  
  Программа будет работать, если на компьютере достаточно оперативной памяти. Однако обработка может занять больше времени, поэтому можно рассмотреть вариант чтения файла частями или оптимизации алгоритма.
  
- **Для 100 миллиардов слов:**  
  Решение потребует использования более сложных подходов, таких как распределённая обработка данных с помощью технологий MapReduce или Apache Spark. Это выходит за рамки моего текущего уровня, но я осознаю, что для таких объёмов данных нужны специализированные инструменты и архитектура.
